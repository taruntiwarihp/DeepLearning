{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMu6PsyLk0eX/Q/4AwJ6Tb2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taruntiwarihp/DeepLearning/blob/master/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcNPB1ijVFqJ",
        "outputId": "75336bfa-996e-44f1-80d3-5dd63e308846"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import json,urllib\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMazbPvV8zU7"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        # no activation and no softmax\n",
        "        return out\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZWIhOgU8zSu"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize(sentence):\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def stem(word):\n",
        "    return stemmer.stem(word.lower())\n",
        "\n",
        "def bag_of_words(tokenized_sentence, all_words):\n",
        "    \"\"\"\n",
        "    sentence = [\"hello, \"how\", \"are\", \"you\"]\n",
        "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
        "    bag =   [0,     1,       0,   1,    0,      0,       0 ]\n",
        "    \"\"\"\n",
        "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
        "    \n",
        "    bag = np.zeros(len(all_words), dtype = np.float32)\n",
        "    for idx, w in enumerate(all_words):\n",
        "        if w in tokenized_sentence:\n",
        "            bag[idx] = 1.0\n",
        "    return bag\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nlZruL-4-Qh"
      },
      "source": [
        "url =  'https://raw.githubusercontent.com/taruntiwarihp/dataSets/master/CakeGift/intents.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Z2bOnz8zQM",
        "outputId": "cf467fed-ac8f-402d-9b26-2fbade04dd31"
      },
      "source": [
        "# with open('intents.json', 'r') as f:\n",
        "#     intents = json.load(f)\n",
        "data = urllib.request.urlopen(url).read().decode()\n",
        "\n",
        "# parse json object\n",
        "intents = json.loads(data)\n",
        "\n",
        "\n",
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        w = tokenize(pattern)\n",
        "        all_words.extend(w)\n",
        "        xy.append((w, tag))\n",
        "\n",
        "ignore_words = ['?','!','.',',']\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "all_words = sorted(set(all_words))\n",
        "tags  = sorted(set(tags))\n",
        "\n",
        "X_train  = []\n",
        "y_train = []\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    bag = bag_of_words(pattern_sentence, all_words)\n",
        "    X_train.append(bag)\n",
        "    \n",
        "    label = tags.index(tag)\n",
        "    y_train.append(label) # CrossEntropyLoss\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.n_samples = len(X_train)\n",
        "        self.x_data = X_train\n",
        "        self.y_data = y_train\n",
        "        \n",
        "    # dataset[idx]\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "# Hyper paramters\n",
        "batch_size = 8\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "input_size = len(X_train[0])\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1000\n",
        "\n",
        "\n",
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset = dataset, batch_size = batch_size,\n",
        "                        shuffle =True, num_workers=2)\n",
        "\n",
        "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(words)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backward and optimizer step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch +1) % 100 == 0:\n",
        "        # f'epoch {epoch+1}/{num_epochs}, loss={loss.item():.4f}'\n",
        "        print(\"epoch {}/{}, loss={:.4f}.\".format(epoch+1,num_epochs,loss.item()))\n",
        "\n",
        "# print(f'Final loss, loss={loss.item():.4f}')\n",
        "print(\"Final Loss, loss{:.4f}\".format(loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 100/1000, loss=0.9713.\n",
            "epoch 200/1000, loss=0.1942.\n",
            "epoch 300/1000, loss=0.1006.\n",
            "epoch 400/1000, loss=0.0127.\n",
            "epoch 500/1000, loss=0.0057.\n",
            "epoch 600/1000, loss=0.0025.\n",
            "epoch 700/1000, loss=0.0011.\n",
            "epoch 800/1000, loss=0.0022.\n",
            "epoch 900/1000, loss=0.0009.\n",
            "epoch 1000/1000, loss=0.0005.\n",
            "Final Loss, loss0.0005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0gBvotx8zNe",
        "outputId": "278ca8b9-cfbd-44e8-ffcb-b9c1e24c1db1"
      },
      "source": [
        "data = {\n",
        "    \"model_state\":model.state_dict(),\n",
        "    \"input_size\":input_size,\n",
        "    \"output_size\":output_size,\n",
        "    \"hiddent_size\": hidden_size,\n",
        "    \"all_words\":all_words,\n",
        "    \"tags\": tags\n",
        "}\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(\"Traning complete. file saved to\",FILE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traning complete. file saved to data.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEnOFOE88zK8",
        "outputId": "a5957de8-8bc2-4f9a-eabb-efc2bc74c637"
      },
      "source": [
        "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# with open('intents.json','r') as f:\n",
        "#   intents = json.load(f)\n",
        "\n",
        "data = urllib.request.urlopen(url).read().decode()\n",
        "\n",
        "# parse json object\n",
        "intents = json.loads(data)  \n",
        "\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "input_size = data['input_size']\n",
        "hidden_size = data['hiddent_size']\n",
        "output_size = data['output_size']\n",
        "all_words = data['all_words']\n",
        "tags = data['tags']\n",
        "model_state = data['model_state']\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (l1): Linear(in_features=80, out_features=8, bias=True)\n",
              "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (l3): Linear(in_features=8, out_features=12, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO9Te5rf31Mt"
      },
      "source": [
        "# pincode datasets\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/taruntiwarihp/dataSets/master/Chat_bot_pincode.csv\",encoding= 'unicode_escape',index_col=None)\n",
        "df= df.drop(columns = ['Region Name','Circle Name','OfficeType','Division Name','Delivery','District','StateName'])\n",
        "\n",
        "# link datasets\n",
        "dt = pd.read_csv('https://raw.githubusercontent.com/taruntiwarihp/dataSets/master/CakeGift/cakegifts.csv')\n",
        "dt.dropna(subset = [\"Catalog\"], inplace=True)\n",
        "dt['Catalog'] = dt['Catalog'].map(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ4NsjIRzkw7"
      },
      "source": [
        "df['Office Name'] = df['Office Name'].map(lambda x: x.replace('B.O', ''))\n",
        "df['Office Name'] = df['Office Name'].map(lambda x: x.replace('S.O', ''))\n",
        "df['Office Name'] = df['Office Name'].map(lambda x: x.replace('G.P.O.', ''))\n",
        "df['Office Name'] = df['Office Name'].map(lambda x: x.replace('H.O.', ''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoD6t7uv3rlr"
      },
      "source": [
        "def callme():\n",
        "  po = {v: k for v, k in enumerate(pf)}\n",
        "  print(\"Taru: Select your near Post Office (Type Given Number only) - We have Service on this Pincode, Below are our nearest store location to your pincode. If you don't idea of the nearest location. Skip it our team will do this. \\n \")\n",
        "\n",
        "  for i,j in po.items():\n",
        "    print(i,j)\n",
        "\n",
        "  sel = input(\"\\n Enter Number\")\n",
        "  if sel.isdigit() :  \n",
        "    sel = int(sel)\n",
        "    print(\"\\n Taru: You selected {} and Delivery available. \\n\".format(po[sel]))\n",
        "  else:  \n",
        "     print(\"Wrong! Please type numbers only!\")  \n",
        "     callme()\n",
        "     \n",
        "    \n",
        "  sen = input(\"Taru: Please let me know , If you want something else. \\n You: \")\n",
        "  return sen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqyzVLEZyYY0"
      },
      "source": [
        "def cakegifts():\n",
        "  enter = input(\"Enter What you want \").lower()\n",
        "  g = dt['Path'][dt['Catalog'].str.contains(enter)][:1]\n",
        "  if len(g) == 0:\n",
        "    print(\"We don't have this item, Please Search Another one\")\n",
        "  else:\n",
        "    print(g.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPSwPzNrl8j8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fc99ff-e007-4d36-f6ee-d5c49b21ea45"
      },
      "source": [
        "cakegifts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter What you want cake\n",
            " https://www.cakegift.in/round-vanilla-sprinkle...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNdhbEbfivxv"
      },
      "source": [
        "# Chatting "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3yL9WOJ8zE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9193d2a-e07e-4bfc-f959-86634a2ef10a"
      },
      "source": [
        "bot_name = \"Taru\"\n",
        "\n",
        "print(\"Namste !!! We are working everywhere in Pan-india. Just drop you delivery location pincode/postal code to check delivery availability or ask us at Whatsapp Helpline 7726996443.\")\n",
        "while True:\n",
        "  sentence = input('You: ')\n",
        "\n",
        "  for word in sentence.split():\n",
        "    if word.isdigit():\n",
        "      word = int(word)\n",
        "      pf = list(df['Office Name'][df['Pincode'] == word ])\n",
        "      if len(pf) == 0:\n",
        "        print(\"Sorry to inform you, We don't deliver here :(\")\n",
        "        break\n",
        "      sentence = callme()\n",
        "\n",
        "  if sentence =='quit':\n",
        "    break\n",
        "  \n",
        "  sentence = tokenize(sentence)\n",
        "  X = bag_of_words(sentence, all_words)\n",
        "  X = X.reshape(1, X.shape[0])\n",
        "  X = torch.from_numpy(X)\n",
        "\n",
        "  output = model(X)\n",
        "  _, predicted = torch.max(output, dim=1)\n",
        "  tag = tags[predicted.item()]\n",
        "\n",
        "  probs = torch.softmax(output, dim=1)\n",
        "  prob = probs[0][predicted.item()]\n",
        "\n",
        "\n",
        "  if prob.item() >0.75:\n",
        "    for intent in intents[\"intents\"]:\n",
        "      if tag == intent[\"tag\"]:\n",
        "        print(bot_name, \":\",random.choice(intent['responses']))\n",
        "        # print(\"{}:{}\".format(bot_name,random.choice(intent['responses'])))\n",
        "      \n",
        "  else:\n",
        "    print(\"{} I do not understand...contact on WhatsApp 7726996443\".format(bot_name))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namste !!! We are working everywhere in Pan-india. Just drop you delivery location pincode/postal code to check delivery availability or ask us at Whatsapp Helpline 7726996443.\n",
            "You: Hi, How are you Taru\n",
            "Taru : Hi there, what can I do for you?\n",
            "You: i want to check my order details \n",
            "Taru : To check the delivery status-related query please send a Whatsapp message at 9829006028, Our team reply there and resolved your concern only on the WhatsApp. Thanks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsq1LtFxJmeh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}