{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49a6cbe6-f10c-4e80-b77d-47bde3848b8c",
    "_uuid": "f81f56104f6d5fc444447b41ec63aa369fd351a8"
   },
   "source": [
    "# Approaching (Almost) Any NLP Problem on Kaggle\n",
    "\n",
    "In this post I'll talk about approaching natural language processing problems on Kaggle. As an example, we will use the data from this competition. We will create a very basic first model first and then improve it using different other features. We will also see how deep neural networks can be used and end this post with some ideas about ensembling in general.\n",
    "\n",
    "### This covers:\n",
    "- tfidf \n",
    "- count features\n",
    "- logistic regression\n",
    "- naive bayes\n",
    "- svm\n",
    "- xgboost\n",
    "- grid search\n",
    "- word vectors\n",
    "- LSTM\n",
    "- GRU\n",
    "- Ensembling\n",
    "\n",
    "*NOTE*: This notebook is not meant for achieving a very high score on the Leaderboard for this dataset. However, if you follow it properly, you can get a very high score with some tuning. ;)\n",
    "\n",
    "So, without wasting any time, let's start with importing some important python modules that I'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d46ba3fd-26f1-4635-b2f9-fca916ff3066",
    "_uuid": "21f3ccd962d1556dc2346699d45a29e9ef791367",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60326be1-82d1-4677-8ef8-da5b1eac475c",
    "_uuid": "adb496504ab8453ce2b4f91dd6e5f17cbdaf4f68"
   },
   "source": [
    "Let's load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "367e0329-7aeb-4f39-b1a9-d7395bdca993",
    "_uuid": "d6ea63db0ad0db09b25c35601391b71564601699",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/spooky/train.csv')\n",
    "test = pd.read_csv('../input/spooky/test.csv')\n",
    "sample = pd.read_csv('../input/spooky/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e171d134-bb33-4578-800c-2d65c2edf9c1",
    "_uuid": "c02f2a3e039aad543bc789188fe08422dd78f5c0"
   },
   "source": [
    "A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1a9da2ba-2c8c-466d-8ceb-cb1fdb4351a8",
    "_uuid": "0fccc28ade4d126b5279d52b0f24300f8c18e69b",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c1ec2ba0-7bac-4983-8f68-850f26251eb6",
    "_uuid": "6b2ace3ea08492e59402dc2abcf65b99ff1a537e",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a3d6737a-2e2b-4213-abf6-94dbefe3a660",
    "_uuid": "43a424c31b40cc7ba4b622fb915846ae00c1f3b7",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c44d877-8fff-41a8-95dc-cd4ddebfa337",
    "_uuid": "68e1ad174df4fd40bb6965d9d46aaf42b8fa39c8"
   },
   "source": [
    "The problem requires us to predict the author, i.e. EAP, HPL and MWS given the text. In simpler words, text classification with 3 different classes.\n",
    "\n",
    "For this particular problem, Kaggle has specified multi-class log-loss as evaluation metric. This is implemented in the follow way (taken from: https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "deb46a3c-6170-4323-8fac-2710662ae0b9",
    "_uuid": "62cd92e75f858aa7c97234e8267a64b00c6d04d0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b4a37951-7a53-43b9-bb5a-0335f1259be3",
    "_uuid": "14ede2221105fb84bb6b2d3a85f9a1f483e8b124"
   },
   "source": [
    "We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "d59a646d-7739-496c-814f-594d371d76eb",
    "_uuid": "19eb8c10f06df8e0f543ee12f794df5f88b0ff1a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65403e74-091f-43c4-9523-3e15d8a75a1e",
    "_uuid": "4ffd04f40d9e921673d06ad64e01b9a7395d8e76"
   },
   "source": [
    "Before going further it is important that we split the data into training and validation sets. We can do it using `train_test_split` from the `model_selection` module of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "ba8e606d-8dee-495e-8c3f-62aa916e9927",
    "_uuid": "b45676b121e2b719d355619e24cfed13d0d33f74",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "9e2fe6a9-8de0-4bbd-8264-f6b78e7993e2",
    "_uuid": "6c8659049537836fdf00d19d6d656630a306d217",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3db70c26-d684-478a-bcd4-980ed6c6d65b",
    "_uuid": "794fb768f4a8e42c4be4f1dbb27144aae4d00c79"
   },
   "source": [
    "## Building Basic Models\n",
    "\n",
    "Let's start building our very first model. \n",
    "\n",
    "Our very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b387f2af-11b1-455d-ad8d-320ed1005be3",
    "_uuid": "350d453dc982f494c3774dbdcf731d856546d611",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "4106bbd1-dc35-4dc2-bda0-3024d3c056d3",
    "_uuid": "3f5dd9ce043364fc61ba3a30298acd9cb72a2938",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.626 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61b2dcb5-46dd-4666-b27f-a8bb48b07f79",
    "_uuid": "f347acb0850db4d13d355e8238250e64f6fbcde8"
   },
   "source": [
    "And there we go. We have our first model with a multiclass logloss of 0.626.\n",
    "\n",
    "But we are greedy and want a better score. Lets look at the same model with a different data.\n",
    "\n",
    "Instead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "7605227c-6cf5-4e00-bcf9-5781afdcc5ed",
    "_uuid": "b464626583417155f1d746e44a2303f560506706",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "fb008df6-95ba-4f50-bdb0-f18371253d39",
    "_uuid": "e459be88b56b9a238b7e7bddfd52df1576dd7dce",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.528 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be77d9bc-e052-47a5-aec6-12a0e07df407",
    "_uuid": "e2eb975dda65d0e32e1160843c54eba171b93f9b"
   },
   "source": [
    "Aaaaanddddddd Wallah! We just improved our first model by 0.1!!!\n",
    "\n",
    "Next, let's try a very simple model which was quite famous in ancient times - Naive Bayes.\n",
    "\n",
    "Let's see what happens when we use naive bayes on these two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "e306fce8-89f5-4c87-9fb8-e25445f3d778",
    "_uuid": "111c474e5cadf662f8f714d689c1079e2730b15b",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c62901ef-dd3b-43dd-87ee-7d365138878a",
    "_uuid": "a629ee2a38f9944ad874c067a089c9957c780059"
   },
   "source": [
    "Good performance! But the logistic regression on counts is still better! What happens when we use this model on counts data instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "db5ca297-b2a8-47fc-beec-90727e32d169",
    "_uuid": "8a91cb086f7fa95281d4c7a795e60f20a22ba865",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5647bb2d-8d76-4a98-91ec-8b111a7a5b30",
    "_uuid": "0c9a2e87e332839f37e2a53bb88a876bb1520cd7"
   },
   "source": [
    "Whoa! Seems like old stuff still works good!!!! One more ancient algorithms in the list is SVMs. Some people \"love\" SVMs. So, we must try SVM on this dataset.\n",
    "\n",
    "Since SVMs take a lot of time, we will reduce the number of features from the TF-IDF using Singular Value Decomposition before applying SVM. \n",
    "\n",
    "Also, note that before applying SVMs, we *must* standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "ae84d757-601f-4f7f-b53d-6d4f99ae7632",
    "_uuid": "d055b28969acd7a93afbb7c51c187b74e539e01b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "54f2755a-be45-4288-a11f-4e485b39b362",
    "_uuid": "3786fbe2fb370926769f27ef38af948aec0ce78b"
   },
   "source": [
    "Now it's time to apply SVM. After running the following cell, feel free to go for a walk or talk to your girlfriend/boyfriend. :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "93f65421-ccde-4711-a292-63ce5d8a1f5f",
    "_uuid": "293f90a7524ab90753ecae42244d1337799740dd",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.741 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f34d307a-ddc4-4c7e-9034-2ffc59cd73d3",
    "_uuid": "534f762addade24a91d0c37cebb195e510e1a6cd"
   },
   "source": [
    "Oops! time to get up! Looks like SVM doesn't perform well on this data...! \n",
    "\n",
    "Before moving further, lets apply the most popular algorithm on Kaggle: xgboost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b8bf1058-a784-4d9e-b55b-b33b86bdccde",
    "_uuid": "b896de07f6a4901c5b8896f94edf0ddbdf072137",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.782 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "e774a72f-a605-4cce-b68f-0005dd260c58",
    "_uuid": "9c62f03cadaa936f5cba3936e8a05697009c2575",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.772 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "5aa042f9-7871-4a2a-bfb4-359b39d7df39",
    "_uuid": "df266177145ab4ad9c291bbb3874cff687def3b0",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.768 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "39d1153f-f890-4a37-8d99-317c9bf84c6e",
    "_uuid": "43042fb84ef91e317cfcfa76cbed3d7a02439a03",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.816 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ed048511-1fbb-46d8-8e83-8e4f2198f30f",
    "_uuid": "49d47f7751496c1b8967ff00158df82a0e5e9f18"
   },
   "source": [
    "Seems like no luck with XGBoost! But that is not correct. I haven't done any hyperparameter optimizations yet. And since I'm lazy, I'll just tell you how to do it and you can do it on your own! ;). This will be discussed in the next section:\n",
    "\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Its a technique for hyperparameter optimization. Not so effective but can give good results if you know the grid you want to use. I specify the parameters that should usually be used in this post: http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/ Please keep in mind that these are the parameters I usually use. There are many other methods of hyperparameter optimization which may or may not be as effective.\n",
    "\n",
    "In this section, I'll talk about grid search using logistic regression. \n",
    "\n",
    "Before starting with grid search we need to create a scoring function. This is accomplished using the `make_scorer` function of scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "50a7703b-52e4-4057-8af7-0d0492f05450",
    "_uuid": "7c3148c857c62662522fddd37d48642526f878da",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "93caaa31-acfc-456d-a991-e37e0279b88f",
    "_uuid": "ee1ec5f4ec8d8db4c888124f63b9aa3ccb5ca736"
   },
   "source": [
    "Next we need a pipeline. For demonstration here, i'll be using a pipeline consisting of SVD, scaling and then logistic regression. Its better to understand with more modules in pipeline than just one ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "0fd480dd-71da-4efc-a8e0-cabc3ab75110",
    "_uuid": "26404a6fbd2dbd045bd4af3fdc6317f6ca1f789f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60857c91-6555-473f-a21d-4ddf565b14d7",
    "_uuid": "343935d35ebf9f36d7be0231fe1cbe9d567be48d"
   },
   "source": [
    "Next we need a grid of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "0d935141-9595-4334-90e5-8ba00281d94c",
    "_uuid": "b3704ca308ccf25b3ea169f923cb0e57cc66f2b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9acb22f4-31d3-436c-bdeb-cf7ba6156ae3",
    "_uuid": "c36718c732d600c08ca82718956e8933183ccd09"
   },
   "source": [
    "So, for SVD we evaluate 120 and 180 components and for logistic regression we evaluate three different values of C with l1 and l2 penalty. We can now start grid search on these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "af0549e8-dc18-4006-a859-af0e36724d1d",
    "_uuid": "10aec7cf5d8c3a1d4ccaf514302c1866eea1b152",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=120 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=120 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=120, score=-0.7804931489517771, total=   8.1s\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=120, score=-0.7773110828018999, total=   8.6s\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=180, score=-0.7403734685802037, total=  11.6s\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=180, score=-0.7530592760423905, total=  12.1s\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=120, score=-0.7780631235604377, total=   5.9s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   14.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=120, score=-0.7709614491211109, total=   7.3s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=120 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=120, score=-0.7808630326486214, total=   7.9s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=180, score=-0.7402371258171362, total=  10.7s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=180, score=-0.7381020449325959, total=  10.4s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=120, score=-0.7745085489773652, total=   9.1s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   25.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=120, score=-0.777767999199495, total=   8.3s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=120, score=-0.7701681555247908, total=   6.8s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=180, score=-0.7484653413704708, total=  13.5s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=120 .................\n",
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=180, score=-0.7376725010243801, total=  16.8s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=120 .................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=180, score=-0.7429854318786947, total=   9.9s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=180 .................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=180, score=-0.7503664045559034, total=  10.0s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=180 .................\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=120, score=-0.7730030417797956, total=   8.0s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=120 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   44.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=120, score=-0.7733682024358349, total=   8.8s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=120 .................\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=120, score=-0.7716823924727487, total=   7.7s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=180 .................\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=120, score=-0.7820380653731355, total=   6.3s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=180 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:   55.0s remaining:   11.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=180, score=-0.7403738704227489, total=  14.0s\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=180, score=-0.7404993394445564, total=  13.9s\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=180, score=-0.7514207456110993, total=   7.3s\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=180, score=-0.7438499331503302, total=   5.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.739\n",
      "Best parameters set:\n",
      "\tlr__C: 0.1\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3bd30250-1894-4fbc-9306-bb2544d5ae10",
    "_uuid": "8ee28a0f484d2e4c6768f3fcd78dd4a017e013fc"
   },
   "source": [
    "The score comes similar to what we had for SVM. This technique can be used to finetune xgboost or even multinomial naive bayes as below. We will use the tfidf data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "444bb821-e8ce-4efd-ab5c-cdb457a438a0",
    "_uuid": "154fb55edb63f5d0c70471a27b2baaa3fa373efa",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV] nb__alpha=0.001 .................................................\n",
      "[CV] nb__alpha=0.001 .................................................\n",
      "[CV] nb__alpha=0.01 ..................................................\n",
      "[CV] nb__alpha=0.01 ..................................................\n",
      "[CV] ....... nb__alpha=0.001, score=-0.6204702511359115, total=   0.0s\n",
      "[CV] nb__alpha=0.1 ...................................................\n",
      "[CV] ....... nb__alpha=0.001, score=-0.6414540936460588, total=   0.0s\n",
      "[CV] nb__alpha=0.1 ...................................................\n",
      "[CV] ........ nb__alpha=0.01, score=-0.5107778556718855, total=   0.0s\n",
      "[CV] nb__alpha=1 .....................................................\n",
      "[CV] ........ nb__alpha=0.01, score=-0.5229890118962334, total=   0.0s\n",
      "[CV] ......... nb__alpha=0.1, score=-0.4891909737902392, total=   0.0s\n",
      "[CV] nb__alpha=1 .....................................................\n",
      "[CV] nb__alpha=10 ....................................................\n",
      "[CV] ......... nb__alpha=0.1, score=-0.4948794942950195, total=   0.0s\n",
      "[CV] ........... nb__alpha=1, score=-0.6629528316617189, total=   0.0s\n",
      "[CV] ............ nb__alpha=1, score=-0.666314466372518, total=   0.0s\n",
      "[CV] .......... nb__alpha=10, score=-0.9496648029421535, total=   0.0s\n",
      "[CV] nb__alpha=10 ....................................................\n",
      "[CV] ........... nb__alpha=10, score=-0.950588400704189, total=   0.0s\n",
      "[CV] nb__alpha=100 ...................................................\n",
      "[CV] .......... nb__alpha=100, score=-1.067264698247982, total=   0.0s\n",
      "[CV] nb__alpha=100 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0769s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.2s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......... nb__alpha=100, score=-1.0673577682186433, total=   0.0s\n",
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01cee065-8733-44d6-b348-7124e178c6e5",
    "_uuid": "ecd0f67a617ae3c03f2386dd722be343c2384ff3"
   },
   "source": [
    "This is an improvement of 8% over the original naive bayes score!\n",
    "\n",
    "In NLP problems, it's customary to look at word vectors. Word vectors give a lot of insights about the data. Let's dive into that.\n",
    "\n",
    "## Word Vectors\n",
    "\n",
    "Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it. I am a fan of GloVe vectors, word2vec and fasttext. In this post, I'll be using the GloVe vectors. You can download the GloVe vectors from here `http://www-nlp.stanford.edu/data/glove.840B.300d.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "a815fde2-a243-4216-929c-2f134ef0663c",
    "_uuid": "8a2edafbcc6c9c0da2ccc1307f7e4ba75950870b",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51998it [00:06, 7559.53it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a29c31599c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '.'"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../input/glove840b300dtxt/glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "fe4e61d9-28d9-4171-a589-b1effaaa1116",
    "_uuid": "a7445bd63a857a4619043f8916384d038a9f5c54",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "e79158dc-eb32-416a-ae91-d38612fca82f",
    "_uuid": "c45f5aeb64f55b6affa932b2f0048b058264518e",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17621 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5a236a709b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create sentence vectors using the above function for training and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxtrain_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mxvalid_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5a236a709b28>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create sentence vectors using the above function for training and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxtrain_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mxvalid_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-5a4af304dd95>\u001b[0m in \u001b[0;36msent2vec\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this function creates a normalized vector for the whole sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "ac1c5c55-a5ca-4f1f-a5cc-ff0964f887d8",
    "_uuid": "e25a2275ca165dc4e90589f4778c9b84585448a1",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d5de355e50ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxtrain_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxvalid_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xtrain_glove' is not defined"
     ]
    }
   ],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9f2b9d5f-d31c-4ed2-ba7f-43833bd63ddd",
    "_uuid": "a295bf47c480c9112dee20e5691594b379f4512e"
   },
   "source": [
    "Let's see the performance of xgboost on glove features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "fb5bb25f-eac2-457d-b69d-e236b5d47953",
    "_uuid": "e35e50e524ae353dda1614c758ff59901e5af23c",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fcd40d899d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fitting a simple xgboost on glove features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xtrain_glove' is not defined"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "41fa545d-0576-4539-bc8c-fb03aef61440",
    "_uuid": "8ab2c7e8eb07e9eb6b7b21d36dcd9b366672cac2",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3c77f32aea02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n\u001b[1;32m      3\u001b[0m                         subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xtrain_glove' is not defined"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "12187a0c-ec9d-46ba-84bb-d99ff9850cf0",
    "_uuid": "f2e77eaf67811c2b8e1465974030f2797ba786ba"
   },
   "source": [
    "we see that a simple tuning of parameters can improve xgboost score on GloVe features! Believe me you can squeeze a lot more from it.\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "But this is an era of deep learning! We cant live without training a few neural networks. Here, we will train LSTM and a simple dense network on the GloVe features. Let's start with the dense network first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "ef1aecb1-2cca-48aa-86f5-ef3f27682924",
    "_uuid": "415001df909a6cdfbb23bd18f042ae98207b736c",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ff512b92f3ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# scale the data before any neural net:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mxtrain_glove_scl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mxvalid_glove_scl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xtrain_glove' is not defined"
     ]
    }
   ],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "e2848ff6-79c2-428e-a932-d9b0295f1d48",
    "_uuid": "bee826d9cf5afb4524d00a539870fc3cff9b601a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "b544f16b-624d-4b21-998e-aecc94295662",
    "_uuid": "d5294e86eeb398dc97a1ff7f2e09a9d26f5ebe16",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "5c443201-164e-4333-92d2-fea861f77609",
    "_uuid": "b7afdb2c41bb7ffc70a18fc65d403c519193647b",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain_glove_scl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-dc5563899340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           validation_data=(xvalid_glove_scl, yvalid_enc))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xtrain_glove_scl' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8b8a0c2-c086-498c-9663-f83b203af00c",
    "_uuid": "bbbd3b800ad85f9f7b3d964b55812f10323b1262"
   },
   "source": [
    "You need to keep on tuning the parameters of the neural network, add more layers, increase dropout to get better results. Here, I'm just showing that its fast to implement and run and gets better result than xgboost without any optimization :)\n",
    "\n",
    "To move further, i.e. with LSTMs we need to tokenize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "35e301c1-a17b-476d-a2c3-b606c9088338",
    "_uuid": "9457e2bf1b79db4a7567c1b04755e66b7828d493",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "9cec8fb1-f218-4d65-aa1c-ab3357cb6b22",
    "_uuid": "63ac5c4c795f45474d263a8e21c01333c2234aa6",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25943 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████▉| 25818/25943 [00:00<00:00, 255556.02it/s]\u001b[A\n",
      "100%|██████████| 25943/25943 [00:00<00:00, 249318.53it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "7f2fa769-d70e-43ef-b6ce-3b82fccaef32",
    "_uuid": "f978f21365eb8bf6684a70e3ae6fae2e8f4fff1b",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method tqdm.__del__ of   0%|          | 0/17621 [00:03<?, ?it/s]>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 882, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 1087, in close\n",
      "    self._decr_instances(self)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 439, in _decr_instances\n",
      "    cls._instances.remove(instance)\n",
      "  File \"/opt/conda/lib/python3.6/_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x7f1fd07ecf48; to 'tqdm' at 0x7f1fd2a966a0>\n"
     ]
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "67593280-81e1-44eb-9cd1-198822f316e8",
    "_uuid": "73d8a4dc4d0e245e979d7d142d1b5c7a4057aa7d",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 37s 2ms/step - loss: 1.0848 - val_loss: 1.0035\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.9756 - val_loss: 0.8530\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.9028 - val_loss: 0.7856\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 40s 2ms/step - loss: 0.8672 - val_loss: 0.7676\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 48s 3ms/step - loss: 0.8423 - val_loss: 0.7559\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 38s 2ms/step - loss: 0.8344 - val_loss: 0.7313\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.8051 - val_loss: 0.7184\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.7929 - val_loss: 0.7109\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.7745 - val_loss: 0.6947\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.7564 - val_loss: 0.6791\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.7387 - val_loss: 0.6756\n",
      "Epoch 12/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.7225 - val_loss: 0.6649\n",
      "Epoch 13/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.7145 - val_loss: 0.6593\n",
      "Epoch 14/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.6930 - val_loss: 0.6491\n",
      "Epoch 15/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.6847 - val_loss: 0.6353\n",
      "Epoch 16/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.6708 - val_loss: 0.6293\n",
      "Epoch 17/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.6609 - val_loss: 0.6275\n",
      "Epoch 18/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.6434 - val_loss: 0.6115\n",
      "Epoch 19/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.6353 - val_loss: 0.6101\n",
      "Epoch 20/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.6227 - val_loss: 0.6152\n",
      "Epoch 21/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.6172 - val_loss: 0.6070\n",
      "Epoch 22/100\n",
      "17621/17621 [==============================] - 60s 3ms/step - loss: 0.5989 - val_loss: 0.6073\n",
      "Epoch 23/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.5876 - val_loss: 0.6042\n",
      "Epoch 24/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.5804 - val_loss: 0.5903\n",
      "Epoch 25/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.5708 - val_loss: 0.5983\n",
      "Epoch 26/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.5593 - val_loss: 0.5941\n",
      "Epoch 27/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.5594 - val_loss: 0.5952\n",
      "Epoch 28/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.5441 - val_loss: 0.5856\n",
      "Epoch 29/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.5328 - val_loss: 0.5863\n",
      "Epoch 30/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.5262 - val_loss: 0.5873\n",
      "Epoch 31/100\n",
      "17621/17621 [==============================] - 39s 2ms/step - loss: 0.5196 - val_loss: 0.5820\n",
      "Epoch 32/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.5142 - val_loss: 0.5795\n",
      "Epoch 33/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.5064 - val_loss: 0.5772\n",
      "Epoch 34/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4959 - val_loss: 0.5948\n",
      "Epoch 35/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4938 - val_loss: 0.5858\n",
      "Epoch 36/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4836 - val_loss: 0.5907\n",
      "Epoch 37/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4815 - val_loss: 0.5909\n",
      "Epoch 38/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4729 - val_loss: 0.5885\n",
      "Epoch 39/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4663 - val_loss: 0.5775\n",
      "Epoch 40/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4643 - val_loss: 0.5778\n",
      "Epoch 41/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4508 - val_loss: 0.5884\n",
      "Epoch 42/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4511 - val_loss: 0.5839\n",
      "Epoch 43/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4490 - val_loss: 0.5811\n",
      "Epoch 44/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4450 - val_loss: 0.5905\n",
      "Epoch 45/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4333 - val_loss: 0.5810\n",
      "Epoch 46/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4269 - val_loss: 0.5956\n",
      "Epoch 47/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4278 - val_loss: 0.5830\n",
      "Epoch 48/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4126 - val_loss: 0.5951\n",
      "Epoch 49/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4213 - val_loss: 0.5852\n",
      "Epoch 50/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4081 - val_loss: 0.6213\n",
      "Epoch 51/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.4042 - val_loss: 0.5976\n",
      "Epoch 52/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4032 - val_loss: 0.6039\n",
      "Epoch 53/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.4024 - val_loss: 0.6172\n",
      "Epoch 54/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3952 - val_loss: 0.5972\n",
      "Epoch 55/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3844 - val_loss: 0.6327\n",
      "Epoch 56/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3857 - val_loss: 0.6081\n",
      "Epoch 57/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3778 - val_loss: 0.5984\n",
      "Epoch 58/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3735 - val_loss: 0.6036\n",
      "Epoch 59/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3714 - val_loss: 0.6057\n",
      "Epoch 60/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3703 - val_loss: 0.6075\n",
      "Epoch 61/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3641 - val_loss: 0.6243\n",
      "Epoch 62/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3656 - val_loss: 0.6310\n",
      "Epoch 63/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3622 - val_loss: 0.6147\n",
      "Epoch 64/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3568 - val_loss: 0.6272\n",
      "Epoch 65/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3562 - val_loss: 0.6275\n",
      "Epoch 66/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3569 - val_loss: 0.6060\n",
      "Epoch 67/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3427 - val_loss: 0.6260\n",
      "Epoch 68/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3398 - val_loss: 0.6336\n",
      "Epoch 69/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3371 - val_loss: 0.6292\n",
      "Epoch 70/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3313 - val_loss: 0.6345\n",
      "Epoch 71/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3397 - val_loss: 0.6274\n",
      "Epoch 72/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3316 - val_loss: 0.6328\n",
      "Epoch 73/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3329 - val_loss: 0.6407\n",
      "Epoch 74/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3237 - val_loss: 0.6368\n",
      "Epoch 75/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3335 - val_loss: 0.6151\n",
      "Epoch 76/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3250 - val_loss: 0.6382\n",
      "Epoch 77/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3182 - val_loss: 0.6354\n",
      "Epoch 78/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3171 - val_loss: 0.6414\n",
      "Epoch 79/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3134 - val_loss: 0.6490\n",
      "Epoch 80/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3103 - val_loss: 0.6473\n",
      "Epoch 81/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3077 - val_loss: 0.6347\n",
      "Epoch 82/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3170 - val_loss: 0.6212\n",
      "Epoch 83/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.3082 - val_loss: 0.6520\n",
      "Epoch 84/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3107 - val_loss: 0.6548\n",
      "Epoch 85/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3038 - val_loss: 0.6479\n",
      "Epoch 86/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.3026 - val_loss: 0.6444\n",
      "Epoch 87/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.2965 - val_loss: 0.6385\n",
      "Epoch 88/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.2938 - val_loss: 0.6643\n",
      "Epoch 89/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.2969 - val_loss: 0.6623\n",
      "Epoch 90/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.2906 - val_loss: 0.6551\n",
      "Epoch 91/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.2901 - val_loss: 0.6664\n",
      "Epoch 92/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.2936 - val_loss: 0.6699\n",
      "Epoch 93/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.2881 - val_loss: 0.6810\n",
      "Epoch 94/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.2838 - val_loss: 0.6723\n",
      "Epoch 95/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.2850 - val_loss: 0.6555\n",
      "Epoch 96/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.2846 - val_loss: 0.6564\n",
      "Epoch 97/100\n",
      "17621/17621 [==============================] - 36s 2ms/step - loss: 0.2750 - val_loss: 0.6659\n",
      "Epoch 98/100\n",
      "17621/17621 [==============================] - 35s 2ms/step - loss: 0.2840 - val_loss: 0.6726\n",
      "Epoch 99/100\n",
      "17621/17621 [==============================] - 41s 2ms/step - loss: 0.2690 - val_loss: 0.6748\n",
      "Epoch 100/100\n",
      "17621/17621 [==============================] - 42s 2ms/step - loss: 0.2745 - val_loss: 0.6530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1fd26a3b38>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d84364cd-85de-4106-91e9-7eca06dab718",
    "_uuid": "01aa520c9ef6283a5399d2dd725d917c67d10213"
   },
   "source": [
    "We see that the score is now less than 0.5. I ran it for many epochs without stopping at the best but you can use early stopping to stop at the best iteration. How do I use early stopping?\n",
    "\n",
    "well, pretty easy. let's compile the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "953a3e6f-0052-4f71-9697-4c3a64846e17",
    "_uuid": "800379d15f6173755b1ec9b2e2d5b5a04509a1b3",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 127s 7ms/step - loss: 1.0599 - val_loss: 0.9231\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 91s 5ms/step - loss: 0.9293 - val_loss: 0.8014\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 89s 5ms/step - loss: 0.8841 - val_loss: 0.7912\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.8527 - val_loss: 0.7835\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 87s 5ms/step - loss: 0.8351 - val_loss: 0.7645\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.8219 - val_loss: 0.7447\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.7980 - val_loss: 0.7195\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 110s 6ms/step - loss: 0.7796 - val_loss: 0.6975\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.7521 - val_loss: 0.7012\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.7344 - val_loss: 0.6821\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.7261 - val_loss: 0.6610\n",
      "Epoch 12/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.6996 - val_loss: 0.6601\n",
      "Epoch 13/100\n",
      "17621/17621 [==============================] - 87s 5ms/step - loss: 0.6880 - val_loss: 0.6474\n",
      "Epoch 14/100\n",
      "17621/17621 [==============================] - 87s 5ms/step - loss: 0.6659 - val_loss: 0.6479\n",
      "Epoch 15/100\n",
      "17621/17621 [==============================] - 87s 5ms/step - loss: 0.6529 - val_loss: 0.6098\n",
      "Epoch 16/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.6335 - val_loss: 0.6254\n",
      "Epoch 17/100\n",
      "17621/17621 [==============================] - 87s 5ms/step - loss: 0.6159 - val_loss: 0.6127\n",
      "Epoch 18/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.6101 - val_loss: 0.6060\n",
      "Epoch 19/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.5820 - val_loss: 0.5935\n",
      "Epoch 20/100\n",
      "17621/17621 [==============================] - 87s 5ms/step - loss: 0.5755 - val_loss: 0.5931\n",
      "Epoch 21/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.5583 - val_loss: 0.5840\n",
      "Epoch 22/100\n",
      "17621/17621 [==============================] - 87s 5ms/step - loss: 0.5402 - val_loss: 0.5922\n",
      "Epoch 23/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.5264 - val_loss: 0.5700\n",
      "Epoch 24/100\n",
      "17621/17621 [==============================] - 87s 5ms/step - loss: 0.5191 - val_loss: 0.5784\n",
      "Epoch 25/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.5045 - val_loss: 0.5784\n",
      "Epoch 26/100\n",
      "17621/17621 [==============================] - 86s 5ms/step - loss: 0.4917 - val_loss: 0.5733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f12f96d30>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf4c30e8-de2f-4787-b5ac-07a22089518e",
    "_uuid": "c2a05818abc8c70e3a8c4ab7118d3815a0b07147"
   },
   "source": [
    "One question could be: why do i use so much dropout? Well, fit the model with no or little dropout and you will that it starts to overfit :)\n",
    "\n",
    "Let's see if Bi-directional LSTM can give us better results. Its a piece of cake to do it with Keras :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "51093329-081b-48d7-b29c-28a0b99f038f",
    "_uuid": "be091fd7888ef0239d7c76536e6a430ca3edca88",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 185s 10ms/step - loss: 1.0608 - val_loss: 0.9236\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 162s 9ms/step - loss: 0.9263 - val_loss: 0.8167\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 162s 9ms/step - loss: 0.8820 - val_loss: 0.7983\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 182s 10ms/step - loss: 0.8574 - val_loss: 0.7951\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 165s 9ms/step - loss: 0.8377 - val_loss: 0.7461\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 163s 9ms/step - loss: 0.8180 - val_loss: 0.7417\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 162s 9ms/step - loss: 0.8020 - val_loss: 0.7259\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 183s 10ms/step - loss: 0.7745 - val_loss: 0.7094\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 164s 9ms/step - loss: 0.7469 - val_loss: 0.6872\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 165s 9ms/step - loss: 0.7349 - val_loss: 0.6730\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 164s 9ms/step - loss: 0.7120 - val_loss: 0.6569\n",
      "Epoch 12/100\n",
      "17621/17621 [==============================] - 188s 11ms/step - loss: 0.6924 - val_loss: 0.6531\n",
      "Epoch 13/100\n",
      "17621/17621 [==============================] - 162s 9ms/step - loss: 0.6728 - val_loss: 0.6292\n",
      "Epoch 14/100\n",
      "17621/17621 [==============================] - 164s 9ms/step - loss: 0.6661 - val_loss: 0.6218\n",
      "Epoch 15/100\n",
      "17621/17621 [==============================] - 165s 9ms/step - loss: 0.6394 - val_loss: 0.6226\n",
      "Epoch 16/100\n",
      "17621/17621 [==============================] - 183s 10ms/step - loss: 0.6279 - val_loss: 0.6055\n",
      "Epoch 17/100\n",
      "17621/17621 [==============================] - 163s 9ms/step - loss: 0.6143 - val_loss: 0.6292\n",
      "Epoch 18/100\n",
      "17621/17621 [==============================] - 163s 9ms/step - loss: 0.5989 - val_loss: 0.5962\n",
      "Epoch 19/100\n",
      "17621/17621 [==============================] - 184s 10ms/step - loss: 0.5831 - val_loss: 0.5833\n",
      "Epoch 20/100\n",
      "17621/17621 [==============================] - 164s 9ms/step - loss: 0.5603 - val_loss: 0.5829\n",
      "Epoch 21/100\n",
      "17621/17621 [==============================] - 165s 9ms/step - loss: 0.5553 - val_loss: 0.5762\n",
      "Epoch 22/100\n",
      "17621/17621 [==============================] - 164s 9ms/step - loss: 0.5425 - val_loss: 0.5734\n",
      "Epoch 23/100\n",
      "17621/17621 [==============================] - 186s 11ms/step - loss: 0.5239 - val_loss: 0.5688\n",
      "Epoch 24/100\n",
      "17621/17621 [==============================] - 163s 9ms/step - loss: 0.5061 - val_loss: 0.5631\n",
      "Epoch 25/100\n",
      "17621/17621 [==============================] - 164s 9ms/step - loss: 0.5028 - val_loss: 0.5747\n",
      "Epoch 26/100\n",
      "17621/17621 [==============================] - 167s 9ms/step - loss: 0.4817 - val_loss: 0.5726\n",
      "Epoch 27/100\n",
      "17621/17621 [==============================] - 182s 10ms/step - loss: 0.4670 - val_loss: 0.5722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1eedc11a20>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "69875648-b716-457a-8db8-bcf625d7d3bf",
    "_uuid": "a1019cf81782debe55d16d55d90b4e1bc3076176"
   },
   "source": [
    "Pretty close! Lets try two layers of GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "e427650c-11e9-422b-9e06-45e353721857",
    "_uuid": "5196303d6875a3d1cd9432631a6a0ecfe0be4d3f",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 146s 8ms/step - loss: 1.0841 - val_loss: 0.9547\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.9777 - val_loss: 0.8823\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.9190 - val_loss: 0.8127\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 163s 9ms/step - loss: 0.8771 - val_loss: 0.8121\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 145s 8ms/step - loss: 0.8438 - val_loss: 0.7713\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.8167 - val_loss: 0.7548\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.7967 - val_loss: 0.7558\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 164s 9ms/step - loss: 0.7760 - val_loss: 0.7120\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.7492 - val_loss: 0.6923\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.7242 - val_loss: 0.6794\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 146s 8ms/step - loss: 0.7072 - val_loss: 0.6438\n",
      "Epoch 12/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.7012 - val_loss: 0.6696\n",
      "Epoch 13/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.6769 - val_loss: 0.6286\n",
      "Epoch 14/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.6551 - val_loss: 0.6275\n",
      "Epoch 15/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.6401 - val_loss: 0.6073\n",
      "Epoch 16/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.6201 - val_loss: 0.5955\n",
      "Epoch 17/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.6100 - val_loss: 0.5868\n",
      "Epoch 18/100\n",
      "17621/17621 [==============================] - 145s 8ms/step - loss: 0.5954 - val_loss: 0.5849\n",
      "Epoch 19/100\n",
      "17621/17621 [==============================] - 142s 8ms/step - loss: 0.5793 - val_loss: 0.5762\n",
      "Epoch 20/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.5639 - val_loss: 0.5731\n",
      "Epoch 21/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.5568 - val_loss: 0.5626\n",
      "Epoch 22/100\n",
      "17621/17621 [==============================] - 143s 8ms/step - loss: 0.5579 - val_loss: 0.5649\n",
      "Epoch 23/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.5294 - val_loss: 0.5600\n",
      "Epoch 24/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.5240 - val_loss: 0.5605\n",
      "Epoch 25/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.5047 - val_loss: 0.5519\n",
      "Epoch 26/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.5021 - val_loss: 0.5728\n",
      "Epoch 27/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.4916 - val_loss: 0.5645\n",
      "Epoch 28/100\n",
      "17621/17621 [==============================] - 162s 9ms/step - loss: 0.4776 - val_loss: 0.5504\n",
      "Epoch 29/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.4760 - val_loss: 0.5417\n",
      "Epoch 30/100\n",
      "17621/17621 [==============================] - 145s 8ms/step - loss: 0.4601 - val_loss: 0.5474\n",
      "Epoch 31/100\n",
      "17621/17621 [==============================] - 144s 8ms/step - loss: 0.4448 - val_loss: 0.5489\n",
      "Epoch 32/100\n",
      "17621/17621 [==============================] - 167s 9ms/step - loss: 0.4396 - val_loss: 0.5448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e9bd77908>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aeb72e91-0b0c-449e-a41b-5d2c1e6a6f26",
    "_uuid": "dfcac382d1c84f19ca12fa3a590ef2c67a30af75"
   },
   "source": [
    "Nice! Much better than what we had previously! Keep optimizing and the performance will keep improving.\n",
    "Worth trying: stemming and lemmatization. This is something I'm skipping for now.\n",
    "\n",
    "In the Kaggle world, to get a top score you should have an ensemble of models. Let's check a little bit of ensembling!\n",
    "\n",
    "\n",
    "## Ensembling\n",
    "\n",
    "Few months back I made a simple ensembler but I didn't have time to develop it fully. It can be found here: https://github.com/abhishekkrthakur/pysembler . I'm going to use some part of it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "074b6b36-c419-4be4-b419-66e00b247f5e",
    "_uuid": "051f7ed5cbace1a16ad6a355bc50293ce13ee5f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the main ensembling class. how to use it is in the next cell!\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
    "                 lower_is_better=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Ensembler init function\n",
    "        :param model_dict: model dictionary, see README for its format\n",
    "        :param num_folds: the number of folds for ensembling\n",
    "        :param task_type: classification or regression\n",
    "        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n",
    "        :param lower_is_better: is lower value of optimization function better or higher\n",
    "        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        \"\"\"\n",
    "        :param training_data: training data in tabular format\n",
    "        :param y: binary, multi-class or regression\n",
    "        :return: chain of models to be used in prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info(\"Found %d classes\", self.num_classes)\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "\n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
    "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
    "\n",
    "        for level in range(self.levels):\n",
    "\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "\n",
    "                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                        (model_num * self.num_classes):(model_num * self.num_classes) +\n",
    "                                                       self.num_classes] = temp_train_predictions\n",
    "\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
    "                                validation_score)\n",
    "                    foldnum += 1\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
    "                            avg_score, std_score)\n",
    "\n",
    "            logger.info(\"Saving predictions for level # %d\", level)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                        index=False, header=None)\n",
    "\n",
    "        return self.train_prediction_dict\n",
    "\n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
    "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
    "        self.test_data = test_data\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "\n",
    "                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "\n",
    "                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n",
    "                                                                                        self.num_classes] = temp_test_predictions\n",
    "\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                       index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "f5498ddf-1cf7-4d67-bdf4-e72d3c59b664",
    "_uuid": "94587d1b4de2770a35027d24c2184982b7b7a34a",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtrain_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-ea981321df0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# specify the data to be used for every level of ensembling:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxtrain_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_ctv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_ctv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxvalid_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_ctv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_ctv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xtrain_glove' is not defined"
     ]
    }
   ],
   "source": [
    "# specify the data to be used for every level of ensembling:\n",
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\n",
    "test_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n",
    "\n",
    "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "\n",
    "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
    "\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n",
    "\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "85e52734-06b5-4025-a9a3-cf9697d66a45",
    "_uuid": "6a1962f3a313d10a6563eb0c8f35d8e7051c3f15",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9dd885242d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmulticlass_logloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# check error:\n",
    "multiclass_logloss(yvalid, preds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "161adbaf-9eec-4e13-87ee-24e4621ee6e0",
    "_uuid": "1ec49abaf6fce8fee5a59f8fc56c85d0ed7626d5"
   },
   "source": [
    "Thus, we see that ensembling improves the score by a great extent! Since this is supposed to be a tutorial only I wont be providing any CSVs that you can submit to the leaderboard.\n",
    "\n",
    "I hope you like it! \n",
    "\n",
    "P.S.: If the response is good, I'll add more stuff in this! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
